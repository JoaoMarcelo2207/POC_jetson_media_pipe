{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'to_categorical' from 'keras.utils' (c:\\Users\\joao.miranda\\AppData\\Local\\miniconda3\\envs\\bio-signals-dataset_env\\lib\\site-packages\\keras\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Keras imports\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping, ReduceLROnPlateau\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Tensorflow Imports\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'to_categorical' from 'keras.utils' (c:\\Users\\joao.miranda\\AppData\\Local\\miniconda3\\envs\\bio-signals-dataset_env\\lib\\site-packages\\keras\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import optuna\n",
    "import uuid\n",
    " \n",
    "# Sklearn imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score, roc_curve\n",
    ")\n",
    " \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    " \n",
    "# Keras imports\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    " \n",
    "# Tensorflow Imports\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Input, BatchNormalization, Attention, LayerNormalization, Lambda, Multiply, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.regularizers import l2, l1\n",
    "from tensorflow.keras import backend as K  # Importando o backend\n",
    " \n",
    " \n",
    "# Import library with current code functions\n",
    "sys.path.append(fr\"C:\\Users\\joao.miranda\\Documents\\POC\\lib\\neural_network_functions.py\")\n",
    "import neural_network_functions as neural_net_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load and clean the dataset\n",
    "def load_and_clean_dataset(dataset_path):\n",
    "    SEQUENCES = pd.read_csv(dataset_path)\n",
    "\n",
    "    if 'Unnamed: 0' in SEQUENCES.columns:\n",
    "        SEQUENCES.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    return SEQUENCES\n",
    "\n",
    "# Filter emotions based on a query\n",
    "def filter_emotions(SEQUENCES, emotions_query):\n",
    "    return SEQUENCES.query(emotions_query)\n",
    "\n",
    "# Encode labels into numerical values\n",
    "def encode_labels(SEQUENCES_ENCODED):\n",
    "    encoder = LabelEncoder()\n",
    "    SEQUENCES_ENCODED['label_numerical'] = encoder.fit_transform(SEQUENCES_ENCODED['label'])\n",
    "    label_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "    print(\"Label Mapping:\", label_mapping)\n",
    "    return SEQUENCES_ENCODED, label_mapping\n",
    "\n",
    "# Select features\n",
    "def select_features(SEQUENCES_ENCODED):\n",
    "    features = SEQUENCES_ENCODED.columns.to_list()[4:26]\n",
    "    print(f\"Features being used: {features}\")\n",
    "    n_features = len(features)\n",
    "    len_sample_max = SEQUENCES_ENCODED.sample_id.value_counts().max()\n",
    "    return features, n_features, len_sample_max\n",
    "\n",
    "# Pad sequences\n",
    "def pad_sequences(SEQUENCES_ENCODED, features, len_sample_max):\n",
    "    grouped_data = []\n",
    "    for sample_id, group in SEQUENCES_ENCODED.groupby('sample_id'):\n",
    "        sequence_features = group[features]\n",
    "        if len(sequence_features) < len_sample_max:\n",
    "            N_rows = len_sample_max - len(sequence_features)\n",
    "            pad = pd.DataFrame(np.zeros((N_rows, len(features))), columns=sequence_features.columns)\n",
    "            sequence_features_pad = pd.concat([pad, sequence_features], ignore_index=True)\n",
    "        else:\n",
    "            sequence_features_pad = sequence_features\n",
    "        label = SEQUENCES_ENCODED[SEQUENCES_ENCODED.sample_id == sample_id].iloc[0].label_numerical\n",
    "        grouped_data.append((sequence_features_pad, label))\n",
    "\n",
    "    X = np.array([item[0] for item in grouped_data])\n",
    "    Y = np.array([item[1] for item in grouped_data])\n",
    "    return X, Y\n",
    "\n",
    "# Normalize data (using external function)\n",
    "def normalize_data(X_balanced):\n",
    "    return neural_net_fun.normalize_data(X_balanced)\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "def split_data(X, Y, train_size=0.7, val_size=0.15, test_size=0.15):\n",
    "    X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size=test_size, stratify=Y, random_state=42, shuffle=True)\n",
    "    val_relative_size = val_size / (train_size + val_size)\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size=val_relative_size, stratify=Y_train_val, random_state=42, shuffle=True)\n",
    "\n",
    "    Y_train = Y_train.reshape(-1, 1)\n",
    "    Y_val = Y_val.reshape(-1, 1)\n",
    "    Y_test = Y_test.reshape(-1, 1)\n",
    "\n",
    "    return X_train, X_val, X_test, Y_train, Y_val, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main preprocessing function\n",
    "def preprocess_data_current_dataset(dataset_path, emotions, emotions_augmentation_factors=None, train_size=0.7, val_size=0.15, test_size=0.15):\n",
    "    SEQUENCES = load_and_clean_dataset(dataset_path)\n",
    "    SEQUENCES = filter_emotions(SEQUENCES, emotions)\n",
    "\n",
    "    SEQUENCES_ENCODED, label_mapping = encode_labels(SEQUENCES)\n",
    "    features, n_features, len_sample_max = select_features(SEQUENCES_ENCODED)\n",
    "    timesteps = len_sample_max\n",
    "    n_classes = len(label_mapping)\n",
    "\n",
    "    # Perform augmentation\n",
    "    if emotions_augmentation_factors:\n",
    "        SEQUENCES_ENCODED_BEFORE = SEQUENCES_ENCODED.copy()  # Save original for comparison\n",
    "        SEQUENCES_ENCODED = perform_augmentation(SEQUENCES_ENCODED, features, emotions_augmentation_factors)\n",
    "\n",
    "    # Padding happens after augmentation\n",
    "    X, Y = pad_sequences(SEQUENCES_ENCODED, features, len_sample_max)\n",
    "    \n",
    "    X_complete = normalize_data(X)\n",
    "\n",
    "    X_train, X_val, X_test, Y_train, Y_val, Y_test = split_data(X_complete, Y, train_size, val_size, test_size)\n",
    "\n",
    "    return {\n",
    "        'label_mapping': label_mapping,\n",
    "        'timesteps': timesteps,\n",
    "        'n_classes': n_classes,\n",
    "        'n_features': n_features,\n",
    "        'X_complete': X_complete,\n",
    "        'Y_complete': Y,\n",
    "        'X_train': X_train,\n",
    "        'X_val': X_val,\n",
    "        'X_test': X_test,\n",
    "        'Y_train': Y_train,\n",
    "        'Y_val': Y_val,\n",
    "        'Y_test': Y_test,\n",
    "        'SEQUENCES_BEFORE': SEQUENCES_ENCODED_BEFORE,  # Data before augmentation\n",
    "        'SEQUENCES_AFTER': SEQUENCES_ENCODED           # Data after augmentation\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio-signals-dataset_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
